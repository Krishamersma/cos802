{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# regex cleaning pattern\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some plot colours\n",
    "colors = ['#142459', '#176BA0', '#19AADE', '#1AC9E6', '#1DE4BD', '#60F0D2', '#c7F9EE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text', 'review/score']].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of rows to sample and default maximum number of aspects:\n",
    "n = 100\n",
    "aspects = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text', 'review/score']].iloc[:n].apply(lambda x: x.astype(str).str.lower())\n",
    "sentence_tokenized = data['review/text'].apply(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sentence cleaning, tokenization and parts of speech tagging function\n",
    "def clean_sentence(sentence):\n",
    "    clean_sentence = re.sub(pattern, ' ', sentence)\n",
    "    token_clean = nltk.word_tokenize(clean_sentence)\n",
    "    pos_clean = nltk.pos_tag(token_clean)\n",
    "    return(pos_clean, clean_sentence, token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and tokenize data\n",
    "review_list = []\n",
    "for review in sentence_tokenized:\n",
    "    sentence_clean = []\n",
    "    sentence_pos = []\n",
    "    sentence_token = []\n",
    "    for sentence in review:\n",
    "        pos, clean, token = clean_sentence(sentence)\n",
    "        sentence_pos.append(pos)\n",
    "        sentence_clean.append(clean)\n",
    "        sentence_token.append(token)\n",
    "    review_dict = {\"sentence\": sentence_clean, \"token\": sentence_token, \"pos\": sentence_pos}\n",
    "    review_list.append(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put tokenized data in dataframe\n",
    "tokenized_data = pd.DataFrame(review_list)   \n",
    "tokenized_data['scores'] = None\n",
    "tokenized_data['duration'] = 0\n",
    "tokenized_data = pd.concat([tokenized_data, data['review/score']], axis=1, join='inner')\n",
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment score calculator using nltk's Sentiment Analyzer\n",
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a sentence, find the dependencies\n",
    "def find_relationships(doc, token, pos):\n",
    "    # categories = []\n",
    "    if doc.sentences[0].dependencies:\n",
    "        dep_obj = []\n",
    "        for dep in doc.sentences[0].dependencies:\n",
    "            dep_obj.append([dep[2].text, dep[0].id, dep[1]])\n",
    "        for item in range(0, len(dep_obj)):\n",
    "            if (int(dep_obj[item][1]) != 0):\n",
    "                dep_obj[item][1] = token[(int(dep_obj[item][1]) - 1)]\n",
    "                \n",
    "        # possible features if noun (NN, NNS), adjective (JJ, JJR) or adverb (RB)\n",
    "        features = []\n",
    "        for item in pos:\n",
    "            # if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "            if item[1] in ('RB', 'NNS', 'JJR', 'NN', 'JJ'):\n",
    "                features.append(list(item))\n",
    "\n",
    "        # cluster together features and descriptive words\n",
    "        cluster = []\n",
    "        for item in features:\n",
    "            feature_list = []\n",
    "            for i in dep_obj:\n",
    "                if((item[0] in (i[0], i[1])) and (i[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                # if((i[0]==item[0] or i[1]==item[0]) and (i[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(i[0]==item[0]):\n",
    "                        feature_list.append(i[1])\n",
    "                    else:\n",
    "                        feature_list.append(i[0])\n",
    "            cluster.append([item[0], feature_list])\n",
    "\n",
    "        # select only nouns\n",
    "        noun_cluster = []\n",
    "        dict = {}\n",
    "        for item in features:\n",
    "            dict[item[0]] = item[1]\n",
    "        for item in cluster:\n",
    "            if(dict[item[0]]==\"NN\"):\n",
    "                noun_cluster.append(item)\n",
    "\n",
    "        # get sentence sentiment scores for all features (aspects) in cluster\n",
    "        sentence_sentiment = sentiment_score(noun_cluster) \n",
    "    return(sentence_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine aspect outputs and scores formatting (untangle lists of lists of lists)\n",
    "def get_aspects(scores):\n",
    "    aspects = []\n",
    "    for x in scores:\n",
    "        for y in x:\n",
    "            aspects.append([y[0][0], y[0][1][0], y[1]])\n",
    "    return(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform absa on given data, with chosen aspect limit and number of samples\n",
    "def absa(tokenized_data, aspects = aspects, n = n):\n",
    "    # loop through data\n",
    "    #review level\n",
    "    for i in tqdm(range(0, n)):\n",
    "        start_review = time.time()\n",
    "        current_aspects = 0\n",
    "        # sentence level\n",
    "        review_scores = []\n",
    "        for j in range(0, len(tokenized_data['sentence'].loc[i]) - 1):\n",
    "            current_aspects = len(review_scores)\n",
    "            if current_aspects >= aspects:\n",
    "                continue\n",
    "            sentence = tokenized_data['sentence'].loc[i][j]\n",
    "            pos = tokenized_data['pos'].loc[i][j]\n",
    "            token = tokenized_data['token'].loc[i][j] \n",
    "            if len(sentence.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                doc = nlp(sentence)\n",
    "                try:\n",
    "                    scores = find_relationships(doc, token, pos)\n",
    "                    if len(scores) != 0:\n",
    "                        review_scores.append(scores)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            # limit number of aspects processed\n",
    "        tokenized_data['scores'].iloc[i] = review_scores[0:aspects]     \n",
    "        duration = time.time() - start_review\n",
    "        tokenized_data['duration'].loc[i] = duration  \n",
    "    return(tokenized_data.iloc[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify review based on aspect sentiment scores\n",
    "def predict(scores):\n",
    "    prediction = None\n",
    "    prediction_sum = 0\n",
    "    for x in scores:\n",
    "        prediction_sum += float(x[2])\n",
    "        if prediction_sum > 0:\n",
    "            prediction = 'Positive'\n",
    "        elif prediction_sum < 0:\n",
    "            prediction = 'Negative'\n",
    "        else:\n",
    "            prediction = 'Neutral'\n",
    "    return(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format and measure output and print to csv\n",
    "def measure(output, n, aspects):\n",
    "    try:\n",
    "        output['review/score'] = output['review/score'].apply(lambda x: int(x.replace('.0', '')))\n",
    "    except:\n",
    "        print('already converted to int!')\n",
    "    output['label'] = np.where(output['review/score'] >= 3, 'Positive', 'Negative')\n",
    "    output['label'] = np.where(output['review/score'] == 3, 'Neutral', output['label'])\n",
    "    try:\n",
    "        output['scores'] = output['scores'].apply(get_aspects)\n",
    "    except:\n",
    "        print('aspects already found!')\n",
    "    output['prediction'] = output['scores'].apply(predict)\n",
    "    output['correct'] = np.where(output['label'] == output['prediction'], 1, 0)\n",
    "    if aspects < 10:\n",
    "        output.to_csv('output/0' + str(aspects) + 'output(n' + str(n) + '_a' + str(aspects) + ').csv')\n",
    "    else:\n",
    "        output.to_csv('output/' + str(aspects) + 'output(n' + str(n) + '_a' + str(aspects) + ').csv')\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform absa for each aspect limit in list (overrides default aspect) and save the output\n",
    "for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "    a = i\n",
    "    print('n =', n, 'a =', a)\n",
    "    output = absa(tokenized_data = tokenized_data, aspects = a, n = n)\n",
    "    measure(output, n, a)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read output into comparitive time_series and percentage of correct predictions\n",
    "time_series = pd.DataFrame(columns = ['a=1', 'a=2', 'a=3', 'a=4', 'a=5', 'a=6', 'a=7', 'a=8', 'a=9', 'a=10', 'a=15', 'a=20'])\n",
    "correct = pd.DataFrame(columns = ['a=1', 'a=2', 'a=3', 'a=4', 'a=5', 'a=6', 'a=7', 'a=8', 'a=9', 'a=10', 'a=15', 'a=20'])\n",
    "correct = []\n",
    "a = 1\n",
    "for filename in os.listdir('output/'):\n",
    "    print(a, filename)\n",
    "    output_file = pd.read_csv('output/' + filename)\n",
    "    series = output_file['duration'].cumsum()\n",
    "    column = 'a=' + str(a)\n",
    "    time_series[column] = series\n",
    "    correct.append(output_file['correct'].sum()/73)\n",
    "    if a< 10:\n",
    "        a += 1\n",
    "    else:\n",
    "        a += 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['a=1', 'a=2', 'a=3', 'a=4', 'a=5', 'a=6', 'a=7', 'a=8', 'a=9', 'a=10', 'a=15', 'a=20']\n",
    "time_series.loc[99].index[0]\n",
    "time_series.loc[99].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series of daily reviews\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(time_series)\n",
    "ax.set_ylabel('Duration')\n",
    "ax.set_xlabel('Percentage of dataset processed')\n",
    "ax.set_title('Duration for different number of aspects')\n",
    "ax.margins(x=0.1)\n",
    "for i in range(0, 3):\n",
    "    plt.text(100, time_series.loc[99].values[i], time_series.loc[99].index[i])\n",
    "# plt.text(100, time_series.loc[99].values[9], time_series.loc[99].index[9])\n",
    "plt.text(100, time_series.loc[99].values[11], time_series.loc[99].index[11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of aspects per review\n",
    "aspect_length= []\n",
    "aspect_df = pd.DataFrame(columns = ['aspects'])\n",
    "for i in range(0, len(output['scores'])):\n",
    "    aspect_length.append(len(output['scores'].loc[i]))\n",
    "aspect_df['aspects'] = aspect_length\n",
    "aspect_df['aspects'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot aspects per review. Make sure that the last asba model run was for a = 20 to show all possible aspects\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(aspect_df.index, aspect_df['aspects'], color = colors[2])\n",
    "ax.set_ylabel('Number of aspects extracted per review')\n",
    "ax.set_xlabel('Number of reviews')\n",
    "ax.set_title('Number of aspects extracted per review')\n",
    "line10, = ax.plot([0, 100], [10, 10], \"c--\", label = 'a=10')\n",
    "line5, = ax.plot([0, 100], [5, 5], \"b--\", label = 'a=5')\n",
    "line4, = ax.plot([0, 100], [4, 4], \"y--\", label = 'a=4')\n",
    "line3, = ax.plot([0, 100], [3, 3], \"g--\", label = 'a=3')\n",
    "line2, = ax.plot([0, 100], [2, 2], \"r--\", label = 'a=2')\n",
    "line1, = ax.plot([0, 100], [1, 1], \"k--\", label = 'a=1')\n",
    "line1.set_label('a=1')\n",
    "line2.set_label('a=2')\n",
    "line3.set_label('a=3')\n",
    "line4.set_label('a=4')\n",
    "line5.set_label('a=5')\n",
    "line10.set_label('a=10')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction accuracy\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(['a=1', 'a=2', 'a=3', 'a=4', 'a=5', 'a=6', 'a=7', 'a=8', 'a=9', 'a=10', 'a=15', 'a=20'], correct, color = colors[0])\n",
    "ax.set_ylabel('Sentiment classification prediciton accuracy')\n",
    "ax.set_xlabel('Number of aspects')\n",
    "ax.set_title('Percentage of correct sentiment classifications per number of allowed aspects (a)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
