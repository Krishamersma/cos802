{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import stanza\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from operator import itemgetter\n",
    "\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'The book is too long, but the characters were good. Who knew Darth Vader was such a caring father?'\n",
    "# sample = 'The book is too long, but the characters were good.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>I'm writng again to say that this book has so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>This book is exciting It's a turn pager you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Ash. Ahh. Such a great guy. Mary-Lynette, she'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>We've fallen head over heels in love with Ash!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Three beautiful sisters (named Rowan, Kestrel ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review/text\n",
       "0    This is only for Julie Strain fans. It's a col...\n",
       "1    I don't care much for Dr. Seuss but after read...\n",
       "2    If people become the books they read and if \"t...\n",
       "3    Theodore Seuss Geisel (1904-1991), aka &quot;D...\n",
       "4    Philip Nel - Dr. Seuss: American IconThis is b...\n",
       "..                                                 ...\n",
       "495  I'm writng again to say that this book has so ...\n",
       "496  This book is exciting It's a turn pager you ca...\n",
       "497  Ash. Ahh. Such a great guy. Mary-Lynette, she'...\n",
       "498  We've fallen head over heels in love with Ash!...\n",
       "499  Three beautiful sisters (named Rowan, Kestrel ...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text']].iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this is only for julie strain fans., it's a c...\n",
       "1    [i don't care much for dr. seuss but after rea...\n",
       "2    [if people become the books they read and if \"...\n",
       "3    [theodore seuss geisel (1904-1991), aka &quot;...\n",
       "4    [philip nel - dr. seuss: american iconthis is ...\n",
       "Name: review/text, dtype: object"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text']].iloc[:500].apply(lambda x: x.astype(str).str.lower())\n",
    "data = data['review/text'].apply(nltk.sent_tokenize)\n",
    "data.head()\n",
    "# text = reviews\n",
    "# text = text.lower()\n",
    "# text = nltk.sent_tokenize(text)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i loved this book for a few reasons the first was that i felt as if i was not reading the story but i was part of the story i loved the way it twisted and turned at every corner i loved that i simply did not know what to expect the choice of words was spectacular and just when i thought that i was in control of my own emotions something would jump up and surprise me i am anxiously waiting for the next book this was one of the best romance suspense books that i have read in years i highly recomend it i give this one five stars '"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(data[22]).replace('[^a-zA-Z0-9]', '')\n",
    "\n",
    "re.sub(pattern, ' ', str(data[22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalfeatureList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score sentiment \n",
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            # print(score['compound'], score['compound'] != 0.0)\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 0 sequence item 0: expected str instance, int found\n",
      "this is only for julie strain fans \n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [314], line 80\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39m# get sentence scores  \u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m sentence_sentiment \u001b[39m=\u001b[39m sentiment_score(finalcluster) \n\u001b[0;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m score \u001b[39min\u001b[39;00m sentence_sentiment:\n",
      "Cell \u001b[1;32mIn [303], line 7\u001b[0m, in \u001b[0;36msentiment_score\u001b[1;34m(finalcluster)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pair[\u001b[39m1\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m     score \u001b[39m=\u001b[39m sid\u001b[39m.\u001b[39mpolarity_scores(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(pair[\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m      8\u001b[0m     \u001b[39m# print(score['compound'], score['compound'] != 0.0)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [314], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[39mprint\u001b[39m(sent)\n\u001b[0;32m     88\u001b[0m         cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 89\u001b[0m         \u001b[39mraise\u001b[39;00m(\u001b[39mNotImplementedError\u001b[39;00m)\n\u001b[0;32m     92\u001b[0m sentence_scores \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(sentence_scores, key\u001b[39m=\u001b[39mitemgetter(\u001b[39m1\u001b[39m), reverse \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     93\u001b[0m \u001b[39m# print('review #:', cnt, sentence_scores)\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "totalfeatureList = []\n",
    "cnt = 0\n",
    "for review in data:\n",
    "    sentence_scores = []\n",
    "    for sent in review:\n",
    "        sent = re.sub(pattern, ' ', sent)\n",
    "        # sent = sent.replace('.', ' ').replace('-', '').replace(\"'\", '').replace(\"/\", '').replace('[^a-zA-Z0-9]', '')\n",
    "        # word_list = nltk.word_tokenize(sent)\n",
    "        # pos_list = nltk.pos_tag(word_list)\n",
    "\n",
    "        # # noun combiner\n",
    "        # newwordList = []\n",
    "        # flag = 0\n",
    "        # for i in range(0,len(pos_list)-1):\n",
    "        #     if(pos_list[i][1]==\"NN\" and pos_list[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "        #         newwordList.append(pos_list[i][0]+pos_list[i+1][0])\n",
    "        #         flag=1\n",
    "        #     else:\n",
    "        #         if(flag==1):\n",
    "        #             flag=0\n",
    "        #             continue\n",
    "        #         newwordList.append(pos_list[i][0])\n",
    "        #         if(i==len(pos_list)-2):\n",
    "        #             newwordList.append(pos_list[i+1][0])\n",
    "        # finaltxt = ' '.join(word for word in newwordList)\n",
    "\n",
    "\n",
    "        # remove stopwords\n",
    "        stop = set(stopwords.words('english'))\n",
    "        new_txt_list = nltk.word_tokenize(sent)\n",
    "        # wordsList = [w for w in new_txt_list if not w in stop]\n",
    "        taggedList = nltk.pos_tag(new_txt_list)\n",
    "\n",
    "        # print(wordsList)\n",
    "\n",
    "        # relationship parser\n",
    "        doc = nlp(sent)\n",
    "        dep_node = []\n",
    "\n",
    "        try:\n",
    "            for dep in doc.sentences[0].dependencies:\n",
    "                dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "            for i in range(0, len(dep_node)):\n",
    "                if (int(dep_node[i][1]) != 0):\n",
    "                    dep_node[i][1] = new_txt_list[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "\n",
    "\n",
    "            # possible features\n",
    "            featureList = []\n",
    "            categories = []\n",
    "            for i in taggedList:\n",
    "                if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                    featureList.append(list(i))\n",
    "                    totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "                    categories.append(i[0])\n",
    "\n",
    "            # cluster together features and descriptors\n",
    "            fcluster = []\n",
    "            for i in featureList:\n",
    "                filist = []\n",
    "                for j in dep_node:\n",
    "                    if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                        if(j[0]==i[0]):\n",
    "                            filist.append(j[1])\n",
    "                        else:\n",
    "                            filist.append(j[0])\n",
    "                fcluster.append([i[0], filist])\n",
    "\n",
    "            # select only nouns\n",
    "            finalcluster = []\n",
    "            dic = {}\n",
    "            for i in featureList:\n",
    "                dic[i[0]] = i[1]\n",
    "            for i in fcluster:\n",
    "                if(dic[i[0]]==\"NN\"):\n",
    "                    finalcluster.append(i)\n",
    "\n",
    "            # get sentence scores  \n",
    "            sentence_sentiment = sentiment_score(finalcluster) \n",
    "            \n",
    "            for score in sentence_sentiment:\n",
    "                sentence_scores.append(score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('review #:', cnt, e) \n",
    "            print(sent)\n",
    "            cnt += 1\n",
    "            raise(NotImplementedError)\n",
    "\n",
    "    \n",
    "    sentence_scores = sorted(sentence_scores, key=itemgetter(1), reverse = True)\n",
    "    # print('review #:', cnt, sentence_scores)\n",
    "    cnt += 1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
