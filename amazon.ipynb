{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text']].iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of rows:\n",
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text']].iloc[:n].apply(lambda x: x.astype(str).str.lower())\n",
    "sentence_tokenized = data['review/text'].apply(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    clean_sentence = re.sub(pattern, ' ', sentence)\n",
    "    token_clean = nltk.word_tokenize(clean_sentence)\n",
    "    pos_clean = nltk.pos_tag(token_clean)\n",
    "    return(pos_clean, clean_sentence, token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "\n",
    "for review in sentence_tokenized:\n",
    "    sentence_clean = []\n",
    "    sentence_pos = []\n",
    "    sentence_token = []\n",
    "    for sentence in review:\n",
    "        pos, clean, token = clean_sentence(sentence)\n",
    "        sentence_pos.append(pos)\n",
    "        sentence_clean.append(clean)\n",
    "        sentence_token.append(token)\n",
    "    review_dict = {\"sentence\": sentence_clean, \"token\": sentence_token, \"pos\": sentence_pos}\n",
    "    review_list.append(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this is only for julie strain fans , it s a c...</td>\n",
       "      <td>[[this, is, only, for, julie, strain, fans], [...</td>\n",
       "      <td>[[(this, DT), (is, VBZ), (only, RB), (for, IN)...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i don t care much for dr seuss but after read...</td>\n",
       "      <td>[[i, don, t, care, much, for, dr, seuss, but, ...</td>\n",
       "      <td>[[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[if people become the books they read and if t...</td>\n",
       "      <td>[[if, people, become, the, books, they, read, ...</td>\n",
       "      <td>[[(if, IN), (people, NNS), (become, VBP), (the...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[theodore seuss geisel 1904 1991 aka quot dr s...</td>\n",
       "      <td>[[theodore, seuss, geisel, 1904, 1991, aka, qu...</td>\n",
       "      <td>[[(theodore, RB), (seuss, JJ), (geisel, NN), (...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[philip nel dr seuss american iconthis is basi...</td>\n",
       "      <td>[[philip, nel, dr, seuss, american, iconthis, ...</td>\n",
       "      <td>[[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [this is only for julie strain fans , it s a c...   \n",
       "1  [i don t care much for dr seuss but after read...   \n",
       "2  [if people become the books they read and if t...   \n",
       "3  [theodore seuss geisel 1904 1991 aka quot dr s...   \n",
       "4  [philip nel dr seuss american iconthis is basi...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [[this, is, only, for, julie, strain, fans], [...   \n",
       "1  [[i, don, t, care, much, for, dr, seuss, but, ...   \n",
       "2  [[if, people, become, the, books, they, read, ...   \n",
       "3  [[theodore, seuss, geisel, 1904, 1991, aka, qu...   \n",
       "4  [[philip, nel, dr, seuss, american, iconthis, ...   \n",
       "\n",
       "                                                 pos scores  \n",
       "0  [[(this, DT), (is, VBZ), (only, RB), (for, IN)...   None  \n",
       "1  [[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...   None  \n",
       "2  [[(if, IN), (people, NNS), (become, VBP), (the...   None  \n",
       "3  [[(theodore, RB), (seuss, JJ), (geisel, NN), (...   None  \n",
       "4  [[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...   None  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = pd.DataFrame(review_list)   \n",
    "tokenized_data['scores'] = None\n",
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score sentiment \n",
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relationships(doc, token, pos):\n",
    "    sentence_scores = []\n",
    "    if doc.sentences[0].dependencies:\n",
    "        dep_node = []\n",
    "        # print(dep_node)\n",
    "        for dep in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = token[(int(dep_node[i][1]) - 1)]\n",
    "                \n",
    "        # possible features\n",
    "        featureList = []\n",
    "        for i in pos:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "\n",
    "        # cluster together features and descriptors\n",
    "        fcluster = []\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "\n",
    "        # select only nouns\n",
    "        finalcluster = []\n",
    "        dic = {}\n",
    "        for i in featureList:\n",
    "            dic[i[0]] = i[1]\n",
    "        for i in fcluster:\n",
    "            if(dic[i[0]]==\"NN\"):\n",
    "                finalcluster.append(i)\n",
    "\n",
    "        # get sentence scores  \n",
    "        sentence_sentiment = sentiment_score(finalcluster) \n",
    "        # for score in sentence_sentiment:\n",
    "        #     sentence_scores.append(score)\n",
    "    return(sentence_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is only for julie strain fans '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['sentence'].loc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n"
     ]
    }
   ],
   "source": [
    "# loop through data\n",
    "review_scores_list = []\n",
    "\n",
    "#reviews\n",
    "for i in range(0, n - 1):\n",
    "    # sentences\n",
    "    review_scores = []\n",
    "    for j in range(0, len(tokenized_data['sentence'].loc[i]) - 1):\n",
    "        sentence_scores = []\n",
    "        sentence = tokenized_data['sentence'].loc[i][j]\n",
    "        pos = tokenized_data['pos'].loc[i][j]\n",
    "        token = tokenized_data['token'].loc[i][j] \n",
    "        if len(sentence.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # print(sentence)\n",
    "            doc = nlp(sentence)\n",
    "            scores = find_relationships(doc, token, pos)\n",
    "            # print(scores)\n",
    "            if len(scores) != 0:\n",
    "                review_scores.append(scores)\n",
    "        tokenized_data['scores'].iloc[i] = review_scores \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 [[[['julie', ['strain']], -0.0516]]]\n",
       "1    [[[['rel', ['plays']], 0.25], [['poet', ['seri...\n",
       "Name: scores, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['scores'].iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[['rel', ['plays']], 0.25], [['poet', ['serious']], -0.0772]]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_sentiment(data):\n",
    "    totalfeatureList = []\n",
    "    cnt = 0\n",
    "    for review in tqdm(data):\n",
    "        sent_cnt = 0\n",
    "        sentence_scores = []\n",
    "        if sent_cnt > 1:\n",
    "            continue\n",
    "        for sent in review:\n",
    "            try:\n",
    "                clean_sentence(sent)\n",
    "                sent = re.sub(pattern, ' ', sent)\n",
    "                # print(sent_cnt, sent)\n",
    "                # clean sentence\n",
    "                sentence_pos, sentence_clean, sentence_token = clean_sentence(sent)\n",
    "\n",
    "                if len(sentence_clean.strip()) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    # relationship parser\n",
    "                    doc = nlp(sentence_clean)\n",
    "                    dep_node = []\n",
    "\n",
    "                    if doc.sentences[0].dependencies:\n",
    "                        for dep in doc.sentences[0].dependencies:\n",
    "                            dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "                        for i in range(0, len(dep_node)):\n",
    "                            if (int(dep_node[i][1]) != 0):\n",
    "                                dep_node[i][1] = sentence_token[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "                        # possible features\n",
    "                        featureList = []\n",
    "                        categories = []\n",
    "                        for i in sentence_pos:\n",
    "                            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                                featureList.append(list(i))\n",
    "                                # totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "                                # categories.append(i[0])\n",
    "\n",
    "                        # cluster together features and descriptors\n",
    "                        fcluster = []\n",
    "                        for i in featureList:\n",
    "                            filist = []\n",
    "                            for j in dep_node:\n",
    "                                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                                    if(j[0]==i[0]):\n",
    "                                        filist.append(j[1])\n",
    "                                    else:\n",
    "                                        filist.append(j[0])\n",
    "                            fcluster.append([i[0], filist])\n",
    "\n",
    "                        # select only nouns\n",
    "                        finalcluster = []\n",
    "                        dic = {}\n",
    "                        for i in featureList:\n",
    "                            dic[i[0]] = i[1]\n",
    "                        for i in fcluster:\n",
    "                            if(dic[i[0]]==\"NN\"):\n",
    "                                finalcluster.append(i)\n",
    "\n",
    "                        # get sentence scores  \n",
    "                        sentence_sentiment = sentiment_score(finalcluster) \n",
    "                        \n",
    "                        for score in sentence_sentiment:\n",
    "                            sentence_scores.append(score)\n",
    "            except Exception as e:\n",
    "                print('review #:', cnt, ' -- skipping sentence', sent_cnt)\n",
    "                continue\n",
    "\n",
    "            \n",
    "\n",
    "            sent_cnt += 1\n",
    "\n",
    "        # sentence_scores = sorted(sentence_scores, key=itemgetter(1), reverse = True)\n",
    "        # print('review #:', cnt, sentence_scores)\n",
    "        # print('review #:', cnt, 'sentence time: ', round(time.time()-review_time, 5))\n",
    "        cnt += 1\n",
    "    return(sentence_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [01:27<02:17,  2.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# run code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m absa \u001b[39m=\u001b[39m aspect_sentiment(data)\n",
      "Cell \u001b[1;32mIn [35], line 21\u001b[0m, in \u001b[0;36maspect_sentiment\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m         \u001b[39m# relationship parser\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         doc \u001b[39m=\u001b[39m nlp(sentence_clean)\n\u001b[0;32m     22\u001b[0m         dep_node \u001b[39m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[39m#         if doc.sentences[0].dependencies:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#             for dep in doc.sentences[0].dependencies:\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#                 dep_node.append([dep[2].text, dep[0].id, dep[1]])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m#             for score in sentence_sentiment:\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m#                 sentence_scores.append(score)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\core.py:408\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\core.py:397\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mget(processor_name):\n\u001b[0;32m    396\u001b[0m         process \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mbulk_process \u001b[39mif\u001b[39;00m bulk \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mprocess\n\u001b[1;32m--> 397\u001b[0m         doc \u001b[39m=\u001b[39m process(doc)\n\u001b[0;32m    398\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\constituency_processor.py:66\u001b[0m, in \u001b[0;36mConstituencyProcessor.process\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tqdm:\n\u001b[0;32m     64\u001b[0m     words \u001b[39m=\u001b[39m tqdm(words)\n\u001b[1;32m---> 66\u001b[0m trees \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mparse_tagged_words(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mmodel, words, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_size)\n\u001b[0;32m     67\u001b[0m document\u001b[39m.\u001b[39mset(CONSTITUENCY, trees, to_sentence\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m document\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:900\u001b[0m, in \u001b[0;36mparse_tagged_words\u001b[1;34m(model, words, batch_size)\u001b[0m\n\u001b[0;32m    897\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    899\u001b[0m sentence_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(words)\n\u001b[1;32m--> 900\u001b[0m treebank \u001b[39m=\u001b[39m parse_sentences(sentence_iterator, build_batch_from_tagged_words, batch_size, model)\n\u001b[0;32m    902\u001b[0m results \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mpredictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtree \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m treebank]\n\u001b[0;32m    903\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:845\u001b[0m, in \u001b[0;36mparse_sentences\u001b[1;34m(data_iterator, build_batch_fn, batch_size, model, best)\u001b[0m\n\u001b[0;32m    843\u001b[0m treebank \u001b[39m=\u001b[39m []\n\u001b[0;32m    844\u001b[0m treebank_indices \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 845\u001b[0m tree_batch \u001b[39m=\u001b[39m build_batch_fn(batch_size, data_iterator, model)\n\u001b[0;32m    846\u001b[0m batch_indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tree_batch)))\n\u001b[0;32m    847\u001b[0m horizon_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m([])\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:823\u001b[0m, in \u001b[0;36mbuild_batch_from_tagged_words\u001b[1;34m(batch_size, data_iterator, model)\u001b[0m\n\u001b[0;32m    820\u001b[0m     tree_batch\u001b[39m.\u001b[39mappend(sentence)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tree_batch) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 823\u001b[0m     tree_batch \u001b[39m=\u001b[39m parse_transitions\u001b[39m.\u001b[39;49minitial_state_from_words(tree_batch, model)\n\u001b[0;32m    824\u001b[0m \u001b[39mreturn\u001b[39;00m tree_batch\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\parse_transitions.py:130\u001b[0m, in \u001b[0;36minitial_state_from_words\u001b[1;34m(word_lists, model)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_state_from_words\u001b[39m(word_lists, model):\n\u001b[0;32m    128\u001b[0m     preterminal_lists \u001b[39m=\u001b[39m [[Tree(tag, Tree(word)) \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m words]\n\u001b[0;32m    129\u001b[0m                          \u001b[39mfor\u001b[39;00m words \u001b[39min\u001b[39;00m word_lists]\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m initial_state_from_preterminals(preterminal_lists, model, gold_trees\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\parse_transitions.py:110\u001b[0m, in \u001b[0;36minitial_state_from_preterminals\u001b[1;34m(preterminal_lists, model, gold_trees)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_state_from_preterminals\u001b[39m(preterminal_lists, model, gold_trees):\n\u001b[0;32m    107\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m    what is passed in should be a list of list of preterminals\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     word_queues \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minitial_word_queues(preterminal_lists)\n\u001b[0;32m    111\u001b[0m     \u001b[39m# this is the bottom of the TreeStack and will be the same for each State\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     transitions\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39minitial_transitions()\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\lstm_model.py:535\u001b[0m, in \u001b[0;36mLSTMModel.initial_word_queues\u001b[1;34m(self, tagged_word_lists)\u001b[0m\n\u001b[0;32m    532\u001b[0m     all_word_inputs\u001b[39m.\u001b[39mappend(word_inputs)\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_charlm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     all_forward_chars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_charlm\u001b[39m.\u001b[39;49mbuild_char_representation(all_word_labels)\n\u001b[0;32m    536\u001b[0m     \u001b[39mfor\u001b[39;00m word_inputs, forward_chars \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(all_word_inputs, all_forward_chars):\n\u001b[0;32m    537\u001b[0m         word_inputs\u001b[39m.\u001b[39mappend(forward_chars)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\char_model.py:179\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    176\u001b[0m chars \u001b[39m=\u001b[39m get_long_tensor(chars, \u001b[39mlen\u001b[39m(all_data), pad_id\u001b[39m=\u001b[39mvocab\u001b[39m.\u001b[39munit2id(CHARLM_END))\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    178\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 179\u001b[0m     output, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(chars, char_lens)\n\u001b[0;32m    180\u001b[0m     res \u001b[39m=\u001b[39m [output[i, offsets] \u001b[39mfor\u001b[39;00m i, offsets \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(char_offsets)]\n\u001b[0;32m    181\u001b[0m     res \u001b[39m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\char_model.py:132\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[1;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m hidden \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \n\u001b[0;32m    130\u001b[0m     hidden \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_h_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous(),\n\u001b[0;32m    131\u001b[0m               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_c_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous())\n\u001b[1;32m--> 132\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcharlstm(embs, charlens, hx\u001b[39m=\u001b[39;49mhidden)\n\u001b[0;32m    133\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m    134\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(output)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[1;34m(self, input, lengths, hx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, PackedSequence):\n\u001b[0;32m     20\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m pack_padded_sequence(\u001b[39minput\u001b[39m, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m---> 22\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx)\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad:\n\u001b[0;32m     24\u001b[0m     res \u001b[39m=\u001b[39m (pad_packed_sequence(res[\u001b[39m0\u001b[39m], batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)[\u001b[39m0\u001b[39m], res[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:777\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[0;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[0;32m    779\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[0;32m    780\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run code\n",
    "absa = aspect_sentiment(data)\n",
    "# absa.head()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
