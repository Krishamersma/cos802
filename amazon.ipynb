{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "import os\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from operator import itemgetter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3080ed81eef45bfae4276a384d61b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 16:36:16 INFO: Downloading default packages for language: en (English) ...\n",
      "2022-11-08 16:36:17 INFO: File exists: C:\\Users\\krish\\stanza_resources\\en\\default.zip\n",
      "2022-11-08 16:36:21 INFO: Finished downloading models and saved to C:\\Users\\krish\\stanza_resources.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 16:36:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d016154fd6442b1b4ef107f67fc73e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 16:36:22 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-11-08 16:36:22 INFO: Use device: cpu\n",
      "2022-11-08 16:36:22 INFO: Loading: tokenize\n",
      "2022-11-08 16:36:22 INFO: Loading: pos\n",
      "2022-11-08 16:36:22 INFO: Loading: lemma\n",
      "2022-11-08 16:36:22 INFO: Loading: depparse\n",
      "2022-11-08 16:36:22 INFO: Loading: sentiment\n",
      "2022-11-08 16:36:23 INFO: Loading: constituency\n",
      "2022-11-08 16:36:23 INFO: Loading: ner\n",
      "2022-11-08 16:36:23 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some nice plot colours\n",
    "colors = ['#142459', '#176BA0', '#19AADE', '#1AC9E6', '#1DE4BD', '#60F0D2', '#c7F9EE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review/text</th>\n",
       "      <th>review/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Dr. Seuss: American Icon\" by Philip Nel is a ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Theodor Seuss Giesel was best known as 'Dr. Se...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>When I recieved this book as a gift for Christ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trams (or any public transport) are not usuall...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As far as I am aware, this is the first book-l...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review/text  review/score\n",
       "0  This is only for Julie Strain fans. It's a col...           4.0\n",
       "1  I don't care much for Dr. Seuss but after read...           5.0\n",
       "2  If people become the books they read and if \"t...           5.0\n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...           4.0\n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b...           4.0\n",
       "5  \"Dr. Seuss: American Icon\" by Philip Nel is a ...           4.0\n",
       "6  Theodor Seuss Giesel was best known as 'Dr. Se...           5.0\n",
       "7  When I recieved this book as a gift for Christ...           5.0\n",
       "8  Trams (or any public transport) are not usuall...           5.0\n",
       "9  As far as I am aware, this is the first book-l...           4.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text', 'review/score']].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of rows:\n",
    "n = 100\n",
    "aspects = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text', 'review/score']].iloc[:n].apply(lambda x: x.astype(str).str.lower())\n",
    "sentence_tokenized = data['review/text'].apply(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    clean_sentence = re.sub(pattern, ' ', sentence)\n",
    "    token_clean = nltk.word_tokenize(clean_sentence)\n",
    "    pos_clean = nltk.pos_tag(token_clean)\n",
    "    return(pos_clean, clean_sentence, token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "review_list = []\n",
    "\n",
    "for review in sentence_tokenized:\n",
    "    sentence_clean = []\n",
    "    sentence_pos = []\n",
    "    sentence_token = []\n",
    "    for sentence in review:\n",
    "        pos, clean, token = clean_sentence(sentence)\n",
    "        sentence_pos.append(pos)\n",
    "        sentence_clean.append(clean)\n",
    "        sentence_token.append(token)\n",
    "    review_dict = {\"sentence\": sentence_clean, \"token\": sentence_token, \"pos\": sentence_pos}\n",
    "    review_list.append(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>scores</th>\n",
       "      <th>duration</th>\n",
       "      <th>review/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this is only for julie strain fans , it s a c...</td>\n",
       "      <td>[[this, is, only, for, julie, strain, fans], [...</td>\n",
       "      <td>[[(this, DT), (is, VBZ), (only, RB), (for, IN)...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i don t care much for dr seuss but after read...</td>\n",
       "      <td>[[i, don, t, care, much, for, dr, seuss, but, ...</td>\n",
       "      <td>[[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[if people become the books they read and if t...</td>\n",
       "      <td>[[if, people, become, the, books, they, read, ...</td>\n",
       "      <td>[[(if, IN), (people, NNS), (become, VBP), (the...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[theodore seuss geisel 1904 1991 aka quot dr s...</td>\n",
       "      <td>[[theodore, seuss, geisel, 1904, 1991, aka, qu...</td>\n",
       "      <td>[[(theodore, RB), (seuss, JJ), (geisel, NN), (...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[philip nel dr seuss american iconthis is basi...</td>\n",
       "      <td>[[philip, nel, dr, seuss, american, iconthis, ...</td>\n",
       "      <td>[[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [this is only for julie strain fans , it s a c...   \n",
       "1  [i don t care much for dr seuss but after read...   \n",
       "2  [if people become the books they read and if t...   \n",
       "3  [theodore seuss geisel 1904 1991 aka quot dr s...   \n",
       "4  [philip nel dr seuss american iconthis is basi...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [[this, is, only, for, julie, strain, fans], [...   \n",
       "1  [[i, don, t, care, much, for, dr, seuss, but, ...   \n",
       "2  [[if, people, become, the, books, they, read, ...   \n",
       "3  [[theodore, seuss, geisel, 1904, 1991, aka, qu...   \n",
       "4  [[philip, nel, dr, seuss, american, iconthis, ...   \n",
       "\n",
       "                                                 pos scores  duration  \\\n",
       "0  [[(this, DT), (is, VBZ), (only, RB), (for, IN)...   None         0   \n",
       "1  [[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...   None         0   \n",
       "2  [[(if, IN), (people, NNS), (become, VBP), (the...   None         0   \n",
       "3  [[(theodore, RB), (seuss, JJ), (geisel, NN), (...   None         0   \n",
       "4  [[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...   None         0   \n",
       "\n",
       "  review/score  \n",
       "0          4.0  \n",
       "1          5.0  \n",
       "2          5.0  \n",
       "3          4.0  \n",
       "4          4.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put tokenized data in dataframe\n",
    "tokenized_data = pd.DataFrame(review_list)   \n",
    "tokenized_data['scores'] = None\n",
    "tokenized_data['duration'] = 0\n",
    "tokenized_data = pd.concat([tokenized_data, data['review/score']], axis=1, join='inner')\n",
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relationships(doc, token, pos):\n",
    "    # categories = []\n",
    "    if doc.sentences[0].dependencies:\n",
    "        dep_node = []\n",
    "        # print(dep_node)\n",
    "        for dep in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = token[(int(dep_node[i][1]) - 1)]\n",
    "                \n",
    "        # possible features\n",
    "        featureList = []\n",
    "        for i in pos:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "\n",
    "        # cluster together features and descriptors\n",
    "        fcluster = []\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "\n",
    "        # select only nouns\n",
    "        finalcluster = []\n",
    "        dic = {}\n",
    "        for i in featureList:\n",
    "            dic[i[0]] = i[1]\n",
    "        for i in fcluster:\n",
    "            if(dic[i[0]]==\"NN\"):\n",
    "                finalcluster.append(i)\n",
    "\n",
    "        # get sentence scores  \n",
    "        sentence_sentiment = sentiment_score(finalcluster) \n",
    "        # for score in sentence_sentiment:\n",
    "        #     sentence_scores.append(score)\n",
    "    return(sentence_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspects(scores):\n",
    "    aspects = []\n",
    "    for x in scores:\n",
    "        for y in x:\n",
    "            aspects.append([y[0][0], y[0][1][0], y[1]])\n",
    "    return(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absa(tokenized_data, aspects = aspects, n = n):\n",
    "    # loop through data\n",
    "    #reviews\n",
    "    for i in tqdm(range(0, n)):\n",
    "        start_review = time.time()\n",
    "        current_aspects = 0\n",
    "        # sentences\n",
    "        review_scores = []\n",
    "        for j in range(0, len(tokenized_data['sentence'].loc[i]) - 1):\n",
    "            current_aspects = len(review_scores)\n",
    "            if current_aspects >= aspects:\n",
    "                continue\n",
    "            sentence = tokenized_data['sentence'].loc[i][j]\n",
    "            pos = tokenized_data['pos'].loc[i][j]\n",
    "            token = tokenized_data['token'].loc[i][j] \n",
    "            if len(sentence.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # print(sentence)\n",
    "                doc = nlp(sentence)\n",
    "                try:\n",
    "                    scores = find_relationships(doc, token, pos)\n",
    "                    # print(scores)\n",
    "                    if len(scores) != 0:\n",
    "                        review_scores.append(scores)\n",
    "                except:\n",
    "                    continue\n",
    "            # limit number of aspects processed\n",
    "        tokenized_data['scores'].iloc[i] = review_scores[0:aspects]     \n",
    "        duration = time.time() - start_review\n",
    "        tokenized_data['duration'].loc[i] = duration  \n",
    "        # break\n",
    "    return(tokenized_data.iloc[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(scores):\n",
    "    prediction = None\n",
    "    prediction_sum = 0\n",
    "    #scores = list(output['scores'].loc[1]\n",
    "    for x in scores:\n",
    "        prediction_sum += float(x[2])\n",
    "        if prediction_sum > 0:\n",
    "            prediction = 'Positive'\n",
    "        elif prediction_sum < 0:\n",
    "            prediction = 'Negative'\n",
    "        else:\n",
    "            prediction = 'Neutral'\n",
    "    return(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(output, n, aspects):\n",
    "    #format and measure output and print to csv\n",
    "    try:\n",
    "        output['review/score'] = output['review/score'].apply(lambda x: int(x.replace('.0', '')))\n",
    "    except:\n",
    "        print('already converted to int!')\n",
    "    output['label'] = np.where(output['review/score'] >= 3, 'Positive', 'Negative')\n",
    "    output['label'] = np.where(output['review/score'] == 3, 'Neutral', output['label'])\n",
    "    try:\n",
    "        output['scores'] = output['scores'].apply(get_aspects)\n",
    "    except:\n",
    "        print('aspects already found!')\n",
    "    output['prediction'] = output['scores'].apply(predict)\n",
    "    output['correct'] = np.where(output['label'] == output['prediction'], 1, 0)\n",
    "    output.to_csv('output/output(n' + str(n) + '_a' + str(aspects) + ').csv')\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [01:23<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:05<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:29<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:36<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:38<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:41<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:44<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:45<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:47<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:46<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:48<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 100 a = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_6556\\2522489009.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tokenized_data['scores'].iloc[i] = review_scores[0:aspects]\n",
      "c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 100/100 [02:48<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "    a = i\n",
    "    print('n =', n, 'a =', a)\n",
    "    output = absa(tokenized_data = tokenized_data, aspects = a, n = n)\n",
    "    measure(output, n, a)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read output into comparitive time_series\n",
    "time_series = pd.DataFrame(columns = ['a=1', 'a=2', 'a=3', 'a=4', 'a=5', 'a=6', 'a=7', 'a=8', 'a=9', 'a=10', 'a=15', 'a=20'])\n",
    "# print(time_series)\n",
    "a = 1\n",
    "for filename in os.listdir('output/'):\n",
    "    output_file = pd.read_csv('output/' + filename)\n",
    "    series = output_file['duration'].cumsum()\n",
    "    column = 'a=' + str(a)\n",
    "    time_series[column] = series\n",
    "    # time_series.append(series.values)\n",
    "    a+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series of daily reviews\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(time_series)\n",
    "ax.set_ylabel('Duration')\n",
    "ax.set_xlabel('Percentage of dataset processed')\n",
    "ax.set_title('Duration for different number of aspects')\n",
    "# print('Max reviews per day: ', max(daily_reviews))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
