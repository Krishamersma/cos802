{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'The book is too long, but the characters were good. Who knew Darth Vader was such a caring father?'\n",
    "# sample = 'The book is too long, but the characters were good.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>I'm writng again to say that this book has so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>This book is exciting It's a turn pager you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Ash. Ahh. Such a great guy. Mary-Lynette, she'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>We've fallen head over heels in love with Ash!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Three beautiful sisters (named Rowan, Kestrel ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review/text\n",
       "0    This is only for Julie Strain fans. It's a col...\n",
       "1    I don't care much for Dr. Seuss but after read...\n",
       "2    If people become the books they read and if \"t...\n",
       "3    Theodore Seuss Geisel (1904-1991), aka &quot;D...\n",
       "4    Philip Nel - Dr. Seuss: American IconThis is b...\n",
       "..                                                 ...\n",
       "495  I'm writng again to say that this book has so ...\n",
       "496  This book is exciting It's a turn pager you ca...\n",
       "497  Ash. Ahh. Such a great guy. Mary-Lynette, she'...\n",
       "498  We've fallen head over heels in love with Ash!...\n",
       "499  Three beautiful sisters (named Rowan, Kestrel ...\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text']].iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of rows:\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this is only for julie strain fans., it's a c...\n",
       "1    [i don't care much for dr. seuss but after rea...\n",
       "2    [if people become the books they read and if \"...\n",
       "3    [theodore seuss geisel (1904-1991), aka &quot;...\n",
       "4    [philip nel - dr. seuss: american iconthis is ...\n",
       "Name: review/text, dtype: object"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text']].iloc[:n].apply(lambda x: x.astype(str).str.lower())\n",
    "data = data['review/text'].apply(nltk.sent_tokenize)\n",
    "data.head()\n",
    "# text = reviews\n",
    "# text = text.lower()\n",
    "# text = nltk.sent_tokenize(text)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tells the wonderful story of how st hyacinth and his fellow dominicans planted the holy catholic faith in poland lithuania russia and all over northern europe \n",
      "many were the remarkable events in this saint s life including the raising of the dead \n",
      "for children ages 10 and up \n",
      "17 illustrations \n",
      "189pp \n",
      "pb \n",
      "imprimatur \n"
     ]
    }
   ],
   "source": [
    "test_data = data[55]\n",
    "for sent in test_data:\n",
    "    print(re.sub(pattern, ' ', sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score sentiment \n",
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    clean_sentence = re.sub(pattern, ' ', sentence)\n",
    "    token_clean = nltk.word_tokenize(clean_sentence)\n",
    "    pos_clean = nltk.pos_tag(token_clean)\n",
    "    return(pos_clean, clean_sentence, token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_sentiment(data):\n",
    "    totalfeatureList = []\n",
    "    cnt = 0\n",
    "    for review in tqdm(data):\n",
    "        sent_cnt = 0\n",
    "        sentence_scores = []\n",
    "        for sent in review:\n",
    "            try:\n",
    "                clean_sentence(sent)\n",
    "                sent = re.sub(pattern, ' ', sent)\n",
    "                # print(sent_cnt, sent)\n",
    "                # clean\n",
    "                sentence_pos, sentence_clean, sentence_token = clean_sentence(sent)\n",
    "\n",
    "                if len(sentence_clean.strip()) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    # relationship parser\n",
    "                    doc = nlp(sent)\n",
    "                    dep_node = []\n",
    "                    if doc.sentences[0].dependencies:\n",
    "                        for dep in doc.sentences[0].dependencies:\n",
    "                            dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "                        for i in range(0, len(dep_node)):\n",
    "                            if (int(dep_node[i][1]) != 0):\n",
    "                                dep_node[i][1] = sentence_token[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "                        # possible features\n",
    "                        featureList = []\n",
    "                        categories = []\n",
    "                        for i in sentence_pos:\n",
    "                            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                                featureList.append(list(i))\n",
    "                                totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "                                categories.append(i[0])\n",
    "\n",
    "                        # cluster together features and descriptors\n",
    "                        fcluster = []\n",
    "                        for i in featureList:\n",
    "                            filist = []\n",
    "                            for j in dep_node:\n",
    "                                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                                    if(j[0]==i[0]):\n",
    "                                        filist.append(j[1])\n",
    "                                    else:\n",
    "                                        filist.append(j[0])\n",
    "                            fcluster.append([i[0], filist])\n",
    "\n",
    "                        # select only nouns\n",
    "                        finalcluster = []\n",
    "                        dic = {}\n",
    "                        for i in featureList:\n",
    "                            dic[i[0]] = i[1]\n",
    "                        for i in fcluster:\n",
    "                            if(dic[i[0]]==\"NN\"):\n",
    "                                finalcluster.append(i)\n",
    "\n",
    "                        # get sentence scores  \n",
    "                        sentence_sentiment = sentiment_score(finalcluster) \n",
    "                        \n",
    "                        for score in sentence_sentiment:\n",
    "                            sentence_scores.append(score)\n",
    "            except Exception as e:\n",
    "                print('review #:', cnt, ' -- skipping sentence', sent_cnt)\n",
    "                continue\n",
    "            sent_cnt += 1\n",
    "\n",
    "        sentence_scores = sorted(sentence_scores, key=itemgetter(1), reverse = True)\n",
    "        # print('review #:', cnt, sentence_scores)\n",
    "        cnt += 1\n",
    "    return(sentence_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:01<09:20,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 0 [[['section', ['nice']], 0.4215], [['book', ['want']], 0.0772], [['julie', ['strain']], -0.0516]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:04<20:57,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 1 [[['book', ['great']], 0.6249], [['work', ['recommend']], 0.3612], [['rel', ['plays']], 0.25], [['poet', ['serious']], -0.0772]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [00:08<27:45,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 2 [[['doctor', ['good']], 0.4404], [['daddy', ['treat']], 0.4019], [['treatment', ['serious']], -0.0772]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [00:16<42:53,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 3 [[['lifelong', ['thrill']], 0.3612], [['nel', ['recommends']], 0.2263], [['lorax', ['protest']], -0.25], [['semitism', ['anti']], -0.3182]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [00:21<39:26,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 4 [[['background', ['enjoy']], 0.4939]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/500 [00:25<38:21,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 5 [[['everyone', ['interested']], 0.4019], [['book', ['thoughtful']], 0.3818], [['i', ['recommend']], 0.3612], [['book', ['recommend']], 0.3612], [['disneyification', ['threatens']], -0.3818]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [00:26<29:09,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review #: 6 [[['giesel', ['created']], 0.25], [['philip', ['argues']], -0.3818]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [00:27<32:42,  3.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [375], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# run code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(aspect_sentiment(data))\n",
      "Cell \u001b[1;32mIn [374], line 19\u001b[0m, in \u001b[0;36maspect_sentiment\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39m# relationship parser\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     doc \u001b[39m=\u001b[39m nlp(sent)\n\u001b[0;32m     20\u001b[0m     dep_node \u001b[39m=\u001b[39m []\n\u001b[0;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m doc\u001b[39m.\u001b[39msentences[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdependencies:\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\core.py:408\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\core.py:397\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mget(processor_name):\n\u001b[0;32m    396\u001b[0m         process \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mbulk_process \u001b[39mif\u001b[39;00m bulk \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mprocess\n\u001b[1;32m--> 397\u001b[0m         doc \u001b[39m=\u001b[39m process(doc)\n\u001b[0;32m    398\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\constituency_processor.py:66\u001b[0m, in \u001b[0;36mConstituencyProcessor.process\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tqdm:\n\u001b[0;32m     64\u001b[0m     words \u001b[39m=\u001b[39m tqdm(words)\n\u001b[1;32m---> 66\u001b[0m trees \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mparse_tagged_words(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mmodel, words, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_size)\n\u001b[0;32m     67\u001b[0m document\u001b[39m.\u001b[39mset(CONSTITUENCY, trees, to_sentence\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m document\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:900\u001b[0m, in \u001b[0;36mparse_tagged_words\u001b[1;34m(model, words, batch_size)\u001b[0m\n\u001b[0;32m    897\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    899\u001b[0m sentence_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(words)\n\u001b[1;32m--> 900\u001b[0m treebank \u001b[39m=\u001b[39m parse_sentences(sentence_iterator, build_batch_from_tagged_words, batch_size, model)\n\u001b[0;32m    902\u001b[0m results \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mpredictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtree \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m treebank]\n\u001b[0;32m    903\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:845\u001b[0m, in \u001b[0;36mparse_sentences\u001b[1;34m(data_iterator, build_batch_fn, batch_size, model, best)\u001b[0m\n\u001b[0;32m    843\u001b[0m treebank \u001b[39m=\u001b[39m []\n\u001b[0;32m    844\u001b[0m treebank_indices \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 845\u001b[0m tree_batch \u001b[39m=\u001b[39m build_batch_fn(batch_size, data_iterator, model)\n\u001b[0;32m    846\u001b[0m batch_indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tree_batch)))\n\u001b[0;32m    847\u001b[0m horizon_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m([])\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:823\u001b[0m, in \u001b[0;36mbuild_batch_from_tagged_words\u001b[1;34m(batch_size, data_iterator, model)\u001b[0m\n\u001b[0;32m    820\u001b[0m     tree_batch\u001b[39m.\u001b[39mappend(sentence)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tree_batch) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 823\u001b[0m     tree_batch \u001b[39m=\u001b[39m parse_transitions\u001b[39m.\u001b[39;49minitial_state_from_words(tree_batch, model)\n\u001b[0;32m    824\u001b[0m \u001b[39mreturn\u001b[39;00m tree_batch\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\parse_transitions.py:130\u001b[0m, in \u001b[0;36minitial_state_from_words\u001b[1;34m(word_lists, model)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_state_from_words\u001b[39m(word_lists, model):\n\u001b[0;32m    128\u001b[0m     preterminal_lists \u001b[39m=\u001b[39m [[Tree(tag, Tree(word)) \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m words]\n\u001b[0;32m    129\u001b[0m                          \u001b[39mfor\u001b[39;00m words \u001b[39min\u001b[39;00m word_lists]\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m initial_state_from_preterminals(preterminal_lists, model, gold_trees\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\parse_transitions.py:110\u001b[0m, in \u001b[0;36minitial_state_from_preterminals\u001b[1;34m(preterminal_lists, model, gold_trees)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_state_from_preterminals\u001b[39m(preterminal_lists, model, gold_trees):\n\u001b[0;32m    107\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m    what is passed in should be a list of list of preterminals\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     word_queues \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minitial_word_queues(preterminal_lists)\n\u001b[0;32m    111\u001b[0m     \u001b[39m# this is the bottom of the TreeStack and will be the same for each State\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     transitions\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39minitial_transitions()\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\lstm_model.py:535\u001b[0m, in \u001b[0;36mLSTMModel.initial_word_queues\u001b[1;34m(self, tagged_word_lists)\u001b[0m\n\u001b[0;32m    532\u001b[0m     all_word_inputs\u001b[39m.\u001b[39mappend(word_inputs)\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_charlm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     all_forward_chars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_charlm\u001b[39m.\u001b[39;49mbuild_char_representation(all_word_labels)\n\u001b[0;32m    536\u001b[0m     \u001b[39mfor\u001b[39;00m word_inputs, forward_chars \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(all_word_inputs, all_forward_chars):\n\u001b[0;32m    537\u001b[0m         word_inputs\u001b[39m.\u001b[39mappend(forward_chars)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\char_model.py:179\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    176\u001b[0m chars \u001b[39m=\u001b[39m get_long_tensor(chars, \u001b[39mlen\u001b[39m(all_data), pad_id\u001b[39m=\u001b[39mvocab\u001b[39m.\u001b[39munit2id(CHARLM_END))\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    178\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 179\u001b[0m     output, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(chars, char_lens)\n\u001b[0;32m    180\u001b[0m     res \u001b[39m=\u001b[39m [output[i, offsets] \u001b[39mfor\u001b[39;00m i, offsets \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(char_offsets)]\n\u001b[0;32m    181\u001b[0m     res \u001b[39m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\char_model.py:132\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[1;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m hidden \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \n\u001b[0;32m    130\u001b[0m     hidden \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_h_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous(),\n\u001b[0;32m    131\u001b[0m               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_c_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous())\n\u001b[1;32m--> 132\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcharlstm(embs, charlens, hx\u001b[39m=\u001b[39;49mhidden)\n\u001b[0;32m    133\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m    134\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(output)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[1;34m(self, input, lengths, hx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, PackedSequence):\n\u001b[0;32m     20\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m pack_padded_sequence(\u001b[39minput\u001b[39m, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m---> 22\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx)\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad:\n\u001b[0;32m     24\u001b[0m     res \u001b[39m=\u001b[39m (pad_packed_sequence(res[\u001b[39m0\u001b[39m], batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)[\u001b[39m0\u001b[39m], res[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:777\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[0;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[0;32m    779\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[0;32m    780\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run code\n",
    "print(aspect_sentiment(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
