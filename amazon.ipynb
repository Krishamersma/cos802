{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc437d4d2bf44f11b5207c250440d3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 20:29:24 INFO: Downloading default packages for language: en (English) ...\n",
      "2022-11-06 20:29:25 INFO: File exists: C:\\Users\\krish\\stanza_resources\\en\\default.zip\n",
      "2022-11-06 20:29:29 INFO: Finished downloading models and saved to C:\\Users\\krish\\stanza_resources.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 20:29:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3310cff459e746fc92d84031a8c6fe7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 20:29:31 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-11-06 20:29:31 INFO: Use device: cpu\n",
      "2022-11-06 20:29:31 INFO: Loading: tokenize\n",
      "2022-11-06 20:29:31 INFO: Loading: pos\n",
      "2022-11-06 20:29:31 INFO: Loading: lemma\n",
      "2022-11-06 20:29:31 INFO: Loading: depparse\n",
      "2022-11-06 20:29:31 INFO: Loading: sentiment\n",
      "2022-11-06 20:29:31 INFO: Loading: constituency\n",
      "2022-11-06 20:29:32 INFO: Loading: ner\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# initialise nlp pipeline\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[39m=\u001b[39m stanza\u001b[39m.\u001b[39;49mPipeline()\n\u001b[0;32m      3\u001b[0m sid \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\core.py:278\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, proxies, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m logger\u001b[39m.\u001b[39mdebug(curr_processor_config)\n\u001b[0;32m    276\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     \u001b[39m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name] \u001b[39m=\u001b[39m NAME_TO_PROCESSOR_CLASS[processor_name](config\u001b[39m=\u001b[39;49mcurr_processor_config,\n\u001b[0;32m    279\u001b[0m                                                                               pipeline\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    280\u001b[0m                                                                               use_gpu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_gpu)\n\u001b[0;32m    281\u001b[0m \u001b[39mexcept\u001b[39;00m ProcessorRequirementsException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    282\u001b[0m     \u001b[39m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     pipeline_reqs_exceptions\u001b[39m.\u001b[39mappend(e)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\processor.py:173\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vocab \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_variant\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_up_model(config, pipeline, use_gpu)\n\u001b[0;32m    175\u001b[0m \u001b[39m# build the final config for the processor\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_up_final_config(config)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\ner_processor.py:49\u001b[0m, in \u001b[0;36mNERProcessor._set_up_model\u001b[1;34m(self, config, pipeline, use_gpu)\u001b[0m\n\u001b[0;32m     46\u001b[0m     pretrain \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mfoundation_cache\u001b[39m.\u001b[39mload_pretrain(pretrain_path) \u001b[39mif\u001b[39;00m pretrain_path \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mcharlm_forward_file\u001b[39m\u001b[39m'\u001b[39m: charlm_forward,\n\u001b[0;32m     48\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcharlm_backward_file\u001b[39m\u001b[39m'\u001b[39m: charlm_backward}\n\u001b[1;32m---> 49\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(args\u001b[39m=\u001b[39;49margs, model_file\u001b[39m=\u001b[39;49mmodel_path, pretrain\u001b[39m=\u001b[39;49mpretrain, use_cuda\u001b[39m=\u001b[39;49muse_gpu, foundation_cache\u001b[39m=\u001b[39;49mpipeline\u001b[39m.\u001b[39;49mfoundation_cache)\n\u001b[0;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainers\u001b[39m.\u001b[39mappend(trainer)\n\u001b[0;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainers[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\ner\\trainer.py:71\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, args, vocab, pretrain, model_file, use_cuda, train_classifier_only, foundation_cache)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_cuda \u001b[39m=\u001b[39m use_cuda\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m model_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[39m# load everything from file\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload(model_file, pretrain, args, foundation_cache)\n\u001b[0;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(var \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m var \u001b[39min\u001b[39;00m [args, vocab, pretrain])\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\ner\\trainer.py:170\u001b[0m, in \u001b[0;36mTrainer.load\u001b[1;34m(self, filename, pretrain, args, foundation_cache)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m pretrain \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     emb_matrix \u001b[39m=\u001b[39m pretrain\u001b[39m.\u001b[39memb\n\u001b[1;32m--> 170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m NERTagger(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab, emb_matrix\u001b[39m=\u001b[39;49memb_matrix, bert_model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_model, bert_tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_tokenizer, use_cuda\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_cuda)\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m], strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    173\u001b[0m \u001b[39m# there is a possible issue with the delta embeddings.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m# specifically, with older models trained without the delta\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39m# embedding matrix\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39m# we need to resave the model with the updated embedding\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# otherwise the resulting model will be broken\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\ner\\model.py:42\u001b[0m, in \u001b[0;36mNERTagger.__init__\u001b[1;34m(self, args, vocab, emb_matrix, bert_model, bert_tokenizer, use_cuda)\u001b[0m\n\u001b[0;32m     39\u001b[0m emb_finetune \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39memb_finetune\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m \u001b[39m# load pretrained embeddings if specified\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m word_emb \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mEmbedding(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab[\u001b[39m'\u001b[39;49m\u001b[39mword\u001b[39;49m\u001b[39m'\u001b[39;49m]), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39mword_emb_dim\u001b[39;49m\u001b[39m'\u001b[39;49m], PAD_ID)\n\u001b[0;32m     43\u001b[0m \u001b[39m# if a model trained with no 'delta' vocab is loaded, and\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m# emb_finetune is off, any resaving of the model will need\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m# the updated vectors.  this is accounted for in load()\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m emb_finetune \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdelta\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab:\n\u001b[0;32m     47\u001b[0m     \u001b[39m# if emb_finetune is off\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[39m# or if the delta embedding is present\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[39m# then we won't fine tune the original embedding\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, device, dtype)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mif\u001b[39;00m _weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlist\u001b[39m(_weight\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[0;32m    145\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:151\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     init\u001b[39m.\u001b[39;49mnormal_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39moverrides\u001b[39m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[39m=\u001b[39mtensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n\u001b[1;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mnormal_(mean, std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'The book is too long, but the characters were good. Who knew Darth Vader was such a caring father?'\n",
    "# sample = 'The book is too long, but the characters were good.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text']].iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of rows:\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this is only for julie strain fans., it's a c...\n",
       "1    [i don't care much for dr. seuss but after rea...\n",
       "2    [if people become the books they read and if \"...\n",
       "3    [theodore seuss geisel (1904-1991), aka &quot;...\n",
       "4    [philip nel - dr. seuss: american iconthis is ...\n",
       "Name: review/text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text']].iloc[:n].apply(lambda x: x.astype(str).str.lower())\n",
    "data = data['review/text'].apply(nltk.sent_tokenize)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this is only for julie strain fans., it's a c...\n",
       "1    [i don't care much for dr. seuss but after rea...\n",
       "2    [if people become the books they read and if \"...\n",
       "3    [theodore seuss geisel (1904-1991), aka &quot;...\n",
       "4    [philip nel - dr. seuss: american iconthis is ...\n",
       "Name: review/text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is only for julie strain fans \n",
      "it s a collection of her photos about 80 pages worth with a nice section of paintings by olivia if you re looking for heavy literary content this isn t the place to find it there s only about 2 pages with text and everything else is photos bottom line if you only want one book the six foot one is probably a better choice however if you like julie like i like julie you won t go wrong on this one either \n"
     ]
    }
   ],
   "source": [
    "test_data = data[0]\n",
    "for sent in test_data:\n",
    "    print(re.sub(pattern, ' ', sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score sentiment \n",
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    clean_time = time.time()\n",
    "    clean_sentence = re.sub(pattern, ' ', sentence)\n",
    "    token_clean = nltk.word_tokenize(clean_sentence)\n",
    "    pos_clean = nltk.pos_tag(token_clean)\n",
    "    # print(round(time.time() - clean_time, 5))\n",
    "    return(pos_clean, clean_sentence, token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_sentiment(data):\n",
    "    totalfeatureList = []\n",
    "    cnt = 0\n",
    "    for review in tqdm(data):\n",
    "        sent_cnt = 0\n",
    "        sentence_scores = []\n",
    "        if sent_cnt > 1:\n",
    "            continue\n",
    "        for sent in review:\n",
    "            try:\n",
    "                clean_sentence(sent)\n",
    "                sent = re.sub(pattern, ' ', sent)\n",
    "                # print(sent_cnt, sent)\n",
    "                # clean sentence\n",
    "                sentence_pos, sentence_clean, sentence_token = clean_sentence(sent)\n",
    "\n",
    "                if len(sentence_clean.strip()) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    # relationship parser\n",
    "                    doc = nlp(sentence_clean)\n",
    "                    dep_node = []\n",
    "\n",
    "            #         if doc.sentences[0].dependencies:\n",
    "            #             for dep in doc.sentences[0].dependencies:\n",
    "            #                 dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "            #             for i in range(0, len(dep_node)):\n",
    "            #                 if (int(dep_node[i][1]) != 0):\n",
    "            #                     dep_node[i][1] = sentence_token[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "            #             # possible features\n",
    "            #             featureList = []\n",
    "            #             categories = []\n",
    "            #             for i in sentence_pos:\n",
    "            #                 if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "            #                     featureList.append(list(i))\n",
    "            #                     totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "            #                     categories.append(i[0])\n",
    "\n",
    "            #             # cluster together features and descriptors\n",
    "            #             fcluster = []\n",
    "            #             for i in featureList:\n",
    "            #                 filist = []\n",
    "            #                 for j in dep_node:\n",
    "            #                     if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "            #                         if(j[0]==i[0]):\n",
    "            #                             filist.append(j[1])\n",
    "            #                         else:\n",
    "            #                             filist.append(j[0])\n",
    "            #                 fcluster.append([i[0], filist])\n",
    "\n",
    "            #             # select only nouns\n",
    "            #             finalcluster = []\n",
    "            #             dic = {}\n",
    "            #             for i in featureList:\n",
    "            #                 dic[i[0]] = i[1]\n",
    "            #             for i in fcluster:\n",
    "            #                 if(dic[i[0]]==\"NN\"):\n",
    "            #                     finalcluster.append(i)\n",
    "\n",
    "            #             # get sentence scores  \n",
    "            #             sentence_sentiment = sentiment_score(finalcluster) \n",
    "                        \n",
    "            #             for score in sentence_sentiment:\n",
    "            #                 sentence_scores.append(score)\n",
    "            except Exception as e:\n",
    "                print('review #:', cnt, ' -- skipping sentence', sent_cnt)\n",
    "                continue\n",
    "\n",
    "            \n",
    "\n",
    "            sent_cnt += 1\n",
    "\n",
    "        # sentence_scores = sorted(sentence_scores, key=itemgetter(1), reverse = True)\n",
    "        # print('review #:', cnt, sentence_scores)\n",
    "        # print('review #:', cnt, 'sentence time: ', round(time.time()-review_time, 5))\n",
    "        cnt += 1\n",
    "    return(sentence_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [01:27<02:17,  2.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# run code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m absa \u001b[39m=\u001b[39m aspect_sentiment(data)\n",
      "Cell \u001b[1;32mIn [35], line 21\u001b[0m, in \u001b[0;36maspect_sentiment\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m         \u001b[39m# relationship parser\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         doc \u001b[39m=\u001b[39m nlp(sentence_clean)\n\u001b[0;32m     22\u001b[0m         dep_node \u001b[39m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[39m#         if doc.sentences[0].dependencies:\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#             for dep in doc.sentences[0].dependencies:\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#                 dep_node.append([dep[2].text, dep[0].id, dep[1]])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m#             for score in sentence_sentiment:\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m#                 sentence_scores.append(score)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\core.py:408\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, doc, processors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(doc, processors)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\core.py:397\u001b[0m, in \u001b[0;36mPipeline.process\u001b[1;34m(self, doc, processors)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors\u001b[39m.\u001b[39mget(processor_name):\n\u001b[0;32m    396\u001b[0m         process \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mbulk_process \u001b[39mif\u001b[39;00m bulk \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessors[processor_name]\u001b[39m.\u001b[39mprocess\n\u001b[1;32m--> 397\u001b[0m         doc \u001b[39m=\u001b[39m process(doc)\n\u001b[0;32m    398\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\pipeline\\constituency_processor.py:66\u001b[0m, in \u001b[0;36mConstituencyProcessor.process\u001b[1;34m(self, document)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tqdm:\n\u001b[0;32m     64\u001b[0m     words \u001b[39m=\u001b[39m tqdm(words)\n\u001b[1;32m---> 66\u001b[0m trees \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mparse_tagged_words(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mmodel, words, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_size)\n\u001b[0;32m     67\u001b[0m document\u001b[39m.\u001b[39mset(CONSTITUENCY, trees, to_sentence\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m document\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:900\u001b[0m, in \u001b[0;36mparse_tagged_words\u001b[1;34m(model, words, batch_size)\u001b[0m\n\u001b[0;32m    897\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m    899\u001b[0m sentence_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(words)\n\u001b[1;32m--> 900\u001b[0m treebank \u001b[39m=\u001b[39m parse_sentences(sentence_iterator, build_batch_from_tagged_words, batch_size, model)\n\u001b[0;32m    902\u001b[0m results \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mpredictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtree \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m treebank]\n\u001b[0;32m    903\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:845\u001b[0m, in \u001b[0;36mparse_sentences\u001b[1;34m(data_iterator, build_batch_fn, batch_size, model, best)\u001b[0m\n\u001b[0;32m    843\u001b[0m treebank \u001b[39m=\u001b[39m []\n\u001b[0;32m    844\u001b[0m treebank_indices \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 845\u001b[0m tree_batch \u001b[39m=\u001b[39m build_batch_fn(batch_size, data_iterator, model)\n\u001b[0;32m    846\u001b[0m batch_indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tree_batch)))\n\u001b[0;32m    847\u001b[0m horizon_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m([])\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\trainer.py:823\u001b[0m, in \u001b[0;36mbuild_batch_from_tagged_words\u001b[1;34m(batch_size, data_iterator, model)\u001b[0m\n\u001b[0;32m    820\u001b[0m     tree_batch\u001b[39m.\u001b[39mappend(sentence)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tree_batch) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 823\u001b[0m     tree_batch \u001b[39m=\u001b[39m parse_transitions\u001b[39m.\u001b[39;49minitial_state_from_words(tree_batch, model)\n\u001b[0;32m    824\u001b[0m \u001b[39mreturn\u001b[39;00m tree_batch\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\parse_transitions.py:130\u001b[0m, in \u001b[0;36minitial_state_from_words\u001b[1;34m(word_lists, model)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_state_from_words\u001b[39m(word_lists, model):\n\u001b[0;32m    128\u001b[0m     preterminal_lists \u001b[39m=\u001b[39m [[Tree(tag, Tree(word)) \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m words]\n\u001b[0;32m    129\u001b[0m                          \u001b[39mfor\u001b[39;00m words \u001b[39min\u001b[39;00m word_lists]\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m initial_state_from_preterminals(preterminal_lists, model, gold_trees\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\parse_transitions.py:110\u001b[0m, in \u001b[0;36minitial_state_from_preterminals\u001b[1;34m(preterminal_lists, model, gold_trees)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_state_from_preterminals\u001b[39m(preterminal_lists, model, gold_trees):\n\u001b[0;32m    107\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m    what is passed in should be a list of list of preterminals\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     word_queues \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minitial_word_queues(preterminal_lists)\n\u001b[0;32m    111\u001b[0m     \u001b[39m# this is the bottom of the TreeStack and will be the same for each State\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     transitions\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39minitial_transitions()\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\constituency\\lstm_model.py:535\u001b[0m, in \u001b[0;36mLSTMModel.initial_word_queues\u001b[1;34m(self, tagged_word_lists)\u001b[0m\n\u001b[0;32m    532\u001b[0m     all_word_inputs\u001b[39m.\u001b[39mappend(word_inputs)\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_charlm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     all_forward_chars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_charlm\u001b[39m.\u001b[39;49mbuild_char_representation(all_word_labels)\n\u001b[0;32m    536\u001b[0m     \u001b[39mfor\u001b[39;00m word_inputs, forward_chars \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(all_word_inputs, all_forward_chars):\n\u001b[0;32m    537\u001b[0m         word_inputs\u001b[39m.\u001b[39mappend(forward_chars)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\char_model.py:179\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    176\u001b[0m chars \u001b[39m=\u001b[39m get_long_tensor(chars, \u001b[39mlen\u001b[39m(all_data), pad_id\u001b[39m=\u001b[39mvocab\u001b[39m.\u001b[39munit2id(CHARLM_END))\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    178\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 179\u001b[0m     output, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(chars, char_lens)\n\u001b[0;32m    180\u001b[0m     res \u001b[39m=\u001b[39m [output[i, offsets] \u001b[39mfor\u001b[39;00m i, offsets \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(char_offsets)]\n\u001b[0;32m    181\u001b[0m     res \u001b[39m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\char_model.py:132\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[1;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m hidden \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \n\u001b[0;32m    130\u001b[0m     hidden \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_h_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous(),\n\u001b[0;32m    131\u001b[0m               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcharlstm_c_init\u001b[39m.\u001b[39mexpand(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_num_layers\u001b[39m\u001b[39m'\u001b[39m], batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mchar_hidden_dim\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcontiguous())\n\u001b[1;32m--> 132\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcharlstm(embs, charlens, hx\u001b[39m=\u001b[39;49mhidden)\n\u001b[0;32m    133\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m    134\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(output)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stanza\\models\\common\\packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[1;34m(self, input, lengths, hx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, PackedSequence):\n\u001b[0;32m     20\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m pack_padded_sequence(\u001b[39minput\u001b[39m, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m---> 22\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx)\n\u001b[0;32m     23\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad:\n\u001b[0;32m     24\u001b[0m     res \u001b[39m=\u001b[39m (pad_packed_sequence(res[\u001b[39m0\u001b[39m], batch_first\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)[\u001b[39m0\u001b[39m], res[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:777\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[0;32m    775\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[0;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    778\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[0;32m    779\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[0;32m    780\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run code\n",
    "absa = aspect_sentiment(data)\n",
    "# absa.head()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
