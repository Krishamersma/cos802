{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "import os\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from operator import itemgetter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa22bd4995041238db0b645670b6f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 22:14:22 INFO: Downloading default packages for language: en (English) ...\n",
      "2022-11-07 22:14:23 INFO: File exists: C:\\Users\\krish\\stanza_resources\\en\\default.zip\n",
      "2022-11-07 22:14:27 INFO: Finished downloading models and saved to C:\\Users\\krish\\stanza_resources.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\krish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 22:14:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a03d57ba5be45b693ee6f1d6b246802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 22:14:46 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-11-07 22:14:46 INFO: Use device: cpu\n",
      "2022-11-07 22:14:46 INFO: Loading: tokenize\n",
      "2022-11-07 22:14:46 INFO: Loading: pos\n",
      "2022-11-07 22:14:46 INFO: Loading: lemma\n",
      "2022-11-07 22:14:46 INFO: Loading: depparse\n",
      "2022-11-07 22:14:47 INFO: Loading: sentiment\n",
      "2022-11-07 22:14:47 INFO: Loading: constituency\n",
      "2022-11-07 22:14:47 INFO: Loading: ner\n",
      "2022-11-07 22:14:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some nice plot colours\n",
    "colors = ['#142459', '#176BA0', '#19AADE', '#1AC9E6', '#1DE4BD', '#60F0D2', '#c7F9EE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reviews \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/Books_rating.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1253\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1251\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1253\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m   1254\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    226\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1038\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1474\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   1430\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1474\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(arr_or_dtype, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, arr_or_dtype)\n\u001b[0;32m   1475\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[0;32m   1476\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review/text</th>\n",
       "      <th>review/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Dr. Seuss: American Icon\" by Philip Nel is a ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Theodor Seuss Giesel was best known as 'Dr. Se...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>When I recieved this book as a gift for Christ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trams (or any public transport) are not usuall...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>As far as I am aware, this is the first book-l...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review/text  review/score\n",
       "0  This is only for Julie Strain fans. It's a col...           4.0\n",
       "1  I don't care much for Dr. Seuss but after read...           5.0\n",
       "2  If people become the books they read and if \"t...           5.0\n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...           4.0\n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b...           4.0\n",
       "5  \"Dr. Seuss: American Icon\" by Philip Nel is a ...           4.0\n",
       "6  Theodor Seuss Giesel was best known as 'Dr. Se...           5.0\n",
       "7  When I recieved this book as a gift for Christ...           5.0\n",
       "8  Trams (or any public transport) are not usuall...           5.0\n",
       "9  As far as I am aware, this is the first book-l...           4.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text', 'review/score']].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of rows:\n",
    "n = 100\n",
    "aspects = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text', 'review/score']].iloc[:n].apply(lambda x: x.astype(str).str.lower())\n",
    "sentence_tokenized = data['review/text'].apply(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    clean_sentence = re.sub(pattern, ' ', sentence)\n",
    "    token_clean = nltk.word_tokenize(clean_sentence)\n",
    "    pos_clean = nltk.pos_tag(token_clean)\n",
    "    return(pos_clean, clean_sentence, token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "review_list = []\n",
    "\n",
    "for review in sentence_tokenized:\n",
    "    sentence_clean = []\n",
    "    sentence_pos = []\n",
    "    sentence_token = []\n",
    "    for sentence in review:\n",
    "        pos, clean, token = clean_sentence(sentence)\n",
    "        sentence_pos.append(pos)\n",
    "        sentence_clean.append(clean)\n",
    "        sentence_token.append(token)\n",
    "    review_dict = {\"sentence\": sentence_clean, \"token\": sentence_token, \"pos\": sentence_pos}\n",
    "    review_list.append(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>scores</th>\n",
       "      <th>duration</th>\n",
       "      <th>review/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this is only for julie strain fans , it s a c...</td>\n",
       "      <td>[[this, is, only, for, julie, strain, fans], [...</td>\n",
       "      <td>[[(this, DT), (is, VBZ), (only, RB), (for, IN)...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i don t care much for dr seuss but after read...</td>\n",
       "      <td>[[i, don, t, care, much, for, dr, seuss, but, ...</td>\n",
       "      <td>[[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[if people become the books they read and if t...</td>\n",
       "      <td>[[if, people, become, the, books, they, read, ...</td>\n",
       "      <td>[[(if, IN), (people, NNS), (become, VBP), (the...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[theodore seuss geisel 1904 1991 aka quot dr s...</td>\n",
       "      <td>[[theodore, seuss, geisel, 1904, 1991, aka, qu...</td>\n",
       "      <td>[[(theodore, RB), (seuss, JJ), (geisel, NN), (...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[philip nel dr seuss american iconthis is basi...</td>\n",
       "      <td>[[philip, nel, dr, seuss, american, iconthis, ...</td>\n",
       "      <td>[[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [this is only for julie strain fans , it s a c...   \n",
       "1  [i don t care much for dr seuss but after read...   \n",
       "2  [if people become the books they read and if t...   \n",
       "3  [theodore seuss geisel 1904 1991 aka quot dr s...   \n",
       "4  [philip nel dr seuss american iconthis is basi...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [[this, is, only, for, julie, strain, fans], [...   \n",
       "1  [[i, don, t, care, much, for, dr, seuss, but, ...   \n",
       "2  [[if, people, become, the, books, they, read, ...   \n",
       "3  [[theodore, seuss, geisel, 1904, 1991, aka, qu...   \n",
       "4  [[philip, nel, dr, seuss, american, iconthis, ...   \n",
       "\n",
       "                                                 pos scores  duration  \\\n",
       "0  [[(this, DT), (is, VBZ), (only, RB), (for, IN)...   None         0   \n",
       "1  [[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...   None         0   \n",
       "2  [[(if, IN), (people, NNS), (become, VBP), (the...   None         0   \n",
       "3  [[(theodore, RB), (seuss, JJ), (geisel, NN), (...   None         0   \n",
       "4  [[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...   None         0   \n",
       "\n",
       "  review/score  \n",
       "0          4.0  \n",
       "1          5.0  \n",
       "2          5.0  \n",
       "3          4.0  \n",
       "4          4.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put tokenized data in dataframe\n",
    "tokenized_data = pd.DataFrame(review_list)   \n",
    "tokenized_data['scores'] = None\n",
    "tokenized_data['duration'] = 0\n",
    "tokenized_data = pd.concat([tokenized_data, data['review/score']], axis=1, join='inner')\n",
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relationships(doc, token, pos):\n",
    "    # categories = []\n",
    "    if doc.sentences[0].dependencies:\n",
    "        dep_node = []\n",
    "        # print(dep_node)\n",
    "        for dep in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = token[(int(dep_node[i][1]) - 1)]\n",
    "                \n",
    "        # possible features\n",
    "        featureList = []\n",
    "        for i in pos:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "\n",
    "        # cluster together features and descriptors\n",
    "        fcluster = []\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "\n",
    "        # select only nouns\n",
    "        finalcluster = []\n",
    "        dic = {}\n",
    "        for i in featureList:\n",
    "            dic[i[0]] = i[1]\n",
    "        for i in fcluster:\n",
    "            if(dic[i[0]]==\"NN\"):\n",
    "                finalcluster.append(i)\n",
    "\n",
    "        # get sentence scores  \n",
    "        sentence_sentiment = sentiment_score(finalcluster) \n",
    "        # for score in sentence_sentiment:\n",
    "        #     sentence_scores.append(score)\n",
    "    return(sentence_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspects(scores):\n",
    "    aspects = []\n",
    "    for x in scores:\n",
    "        for y in x:\n",
    "            aspects.append([y[0][0], y[0][1][0], y[1]])\n",
    "    return(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absa(tokenized_data, aspects = aspects, n = n):\n",
    "    # loop through data\n",
    "    #reviews\n",
    "    for i in tqdm(range(0, n)):\n",
    "        start_review = time.time()\n",
    "        current_aspects = 0\n",
    "        # sentences\n",
    "        review_scores = []\n",
    "        for j in range(0, len(tokenized_data['sentence'].loc[i]) - 1):\n",
    "            current_aspects = len(review_scores)\n",
    "            if current_aspects >= aspects:\n",
    "                continue\n",
    "            sentence = tokenized_data['sentence'].loc[i][j]\n",
    "            pos = tokenized_data['pos'].loc[i][j]\n",
    "            token = tokenized_data['token'].loc[i][j] \n",
    "            if len(sentence.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # print(sentence)\n",
    "                doc = nlp(sentence)\n",
    "                try:\n",
    "                    scores = find_relationships(doc, token, pos)\n",
    "                    # print(scores)\n",
    "                    if len(scores) != 0:\n",
    "                        review_scores.append(scores)\n",
    "                except:\n",
    "                    continue\n",
    "            # limit number of aspects processed\n",
    "        tokenized_data['scores'].iloc[i] = review_scores[0:aspects]     \n",
    "        duration = time.time() - start_review\n",
    "        tokenized_data['duration'].loc[i] = duration  \n",
    "        # break\n",
    "    return(tokenized_data.iloc[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(scores):\n",
    "    prediction = None\n",
    "    prediction_sum = 0\n",
    "    #scores = list(output['scores'].loc[1]\n",
    "    for x in scores:\n",
    "        prediction_sum += float(x[2])\n",
    "        if prediction_sum > 0:\n",
    "            prediction = 'Positive'\n",
    "        elif prediction_sum < 0:\n",
    "            prediction = 'Negative'\n",
    "        else:\n",
    "            prediction = 'Neutral'\n",
    "    return(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(output, n, aspects):\n",
    "    #format and measure output and print to csv\n",
    "    try:\n",
    "        output['review/score'] = output['review/score'].apply(lambda x: int(x.replace('.0', '')))\n",
    "    except:\n",
    "        print('already converted to int!')\n",
    "    output['label'] = np.where(output['review/score'] >= 3, 'Positive', 'Negative')\n",
    "    output['label'] = np.where(output['review/score'] == 3, 'Neutral', output['label'])\n",
    "    try:\n",
    "        output['scores'] = output['scores'].apply(get_aspects)\n",
    "    except:\n",
    "        print('aspects already found!')\n",
    "    output['prediction'] = output['scores'].apply(predict)\n",
    "    output['correct'] = np.where(output['label'] == output['prediction'], 1, 0)\n",
    "    output.to_csv('output/output(n' + str(n) + '_a' + str(aspects) + ').csv')\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m20\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     a \u001b[39m=\u001b[39m i\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mn =\u001b[39m\u001b[39m'\u001b[39m, n, \u001b[39m'\u001b[39m\u001b[39ma =\u001b[39m\u001b[39m'\u001b[39m, a)\n\u001b[0;32m      4\u001b[0m     output \u001b[39m=\u001b[39m absa(tokenized_data \u001b[39m=\u001b[39m tokenized_data, aspects \u001b[39m=\u001b[39m aspects, n \u001b[39m=\u001b[39m n)\n\u001b[0;32m      5\u001b[0m     measure(output, n, a)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "    a = i\n",
    "    print('n =', n, 'a =', a)\n",
    "    output = absa(tokenized_data = tokenized_data, aspects = aspects, n = n)\n",
    "    measure(output, n, a)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read output into comparitive time_series\n",
    "time_series = pd.DataFrame(columns = ['a=1', 'a=2', 'a=3', 'a=4', 'a=5', 'a=6', 'a=7', 'a=8', 'a=9', 'a=10', 'a=15', 'a=20'])\n",
    "# print(time_series)\n",
    "a = 1\n",
    "for filename in os.listdir('output/'):\n",
    "    output_file = pd.read_csv('output/' + filename)\n",
    "    series = output_file['duration'].cumsum()\n",
    "    column = 'a=' + str(a)\n",
    "    time_series[column] = series\n",
    "    # time_series.append(series.values)\n",
    "    a+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series of daily reviews\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(time_series)\n",
    "ax.set_ylabel('Duration')\n",
    "ax.set_xlabel('Percentage of dataset processed')\n",
    "ax.set_title('Duration for different number of aspects')\n",
    "# print('Max reviews per day: ', max(daily_reviews))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
