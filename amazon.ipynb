{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import nltk\n",
    "import stanza\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from operator import itemgetter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "pattern = r'[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text']].iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of rows:\n",
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text']].iloc[:n].apply(lambda x: x.astype(str).str.lower())\n",
    "sentence_tokenized = data['review/text'].apply(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    clean_sentence = re.sub(pattern, ' ', sentence)\n",
    "    token_clean = nltk.word_tokenize(clean_sentence)\n",
    "    pos_clean = nltk.pos_tag(token_clean)\n",
    "    return(pos_clean, clean_sentence, token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = []\n",
    "\n",
    "for review in sentence_tokenized:\n",
    "    sentence_clean = []\n",
    "    sentence_pos = []\n",
    "    sentence_token = []\n",
    "    for sentence in review:\n",
    "        pos, clean, token = clean_sentence(sentence)\n",
    "        sentence_pos.append(pos)\n",
    "        sentence_clean.append(clean)\n",
    "        sentence_token.append(token)\n",
    "    review_dict = {\"sentence\": sentence_clean, \"token\": sentence_token, \"pos\": sentence_pos}\n",
    "    review_list.append(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this is only for julie strain fans , it s a c...</td>\n",
       "      <td>[[this, is, only, for, julie, strain, fans], [...</td>\n",
       "      <td>[[(this, DT), (is, VBZ), (only, RB), (for, IN)...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i don t care much for dr seuss but after read...</td>\n",
       "      <td>[[i, don, t, care, much, for, dr, seuss, but, ...</td>\n",
       "      <td>[[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[if people become the books they read and if t...</td>\n",
       "      <td>[[if, people, become, the, books, they, read, ...</td>\n",
       "      <td>[[(if, IN), (people, NNS), (become, VBP), (the...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[theodore seuss geisel 1904 1991 aka quot dr s...</td>\n",
       "      <td>[[theodore, seuss, geisel, 1904, 1991, aka, qu...</td>\n",
       "      <td>[[(theodore, RB), (seuss, JJ), (geisel, NN), (...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[philip nel dr seuss american iconthis is basi...</td>\n",
       "      <td>[[philip, nel, dr, seuss, american, iconthis, ...</td>\n",
       "      <td>[[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  [this is only for julie strain fans , it s a c...   \n",
       "1  [i don t care much for dr seuss but after read...   \n",
       "2  [if people become the books they read and if t...   \n",
       "3  [theodore seuss geisel 1904 1991 aka quot dr s...   \n",
       "4  [philip nel dr seuss american iconthis is basi...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [[this, is, only, for, julie, strain, fans], [...   \n",
       "1  [[i, don, t, care, much, for, dr, seuss, but, ...   \n",
       "2  [[if, people, become, the, books, they, read, ...   \n",
       "3  [[theodore, seuss, geisel, 1904, 1991, aka, qu...   \n",
       "4  [[philip, nel, dr, seuss, american, iconthis, ...   \n",
       "\n",
       "                                                 pos scores  \n",
       "0  [[(this, DT), (is, VBZ), (only, RB), (for, IN)...   None  \n",
       "1  [[(i, JJ), (don, VBP), (t, EX), (care, NN), (m...   None  \n",
       "2  [[(if, IN), (people, NNS), (become, VBP), (the...   None  \n",
       "3  [[(theodore, RB), (seuss, JJ), (geisel, NN), (...   None  \n",
       "4  [[(philip, NN), (nel, NNS), (dr, VBP), (seuss,...   None  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = pd.DataFrame(review_list)   \n",
    "tokenized_data['scores'] = None\n",
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score sentiment \n",
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            if score['compound'] != 0.0:\n",
    "                pair_score = [pair, score['compound']]\n",
    "                scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relationships(doc, token, pos):\n",
    "    categories = []\n",
    "    if doc.sentences[0].dependencies:\n",
    "        dep_node = []\n",
    "        # print(dep_node)\n",
    "        for dep in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep[2].text, dep[0].id, dep[1]])\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = token[(int(dep_node[i][1]) - 1)]\n",
    "                \n",
    "        # possible features\n",
    "        featureList = []\n",
    "        for i in pos:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "                categories.append(i[0])\n",
    "\n",
    "\n",
    "        # cluster together features and descriptors\n",
    "        fcluster = []\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "\n",
    "        # select only nouns\n",
    "        finalcluster = []\n",
    "        dic = {}\n",
    "        for i in featureList:\n",
    "            dic[i[0]] = i[1]\n",
    "        for i in fcluster:\n",
    "            if(dic[i[0]]==\"NN\"):\n",
    "                finalcluster.append(i)\n",
    "\n",
    "        # get sentence scores  \n",
    "        sentence_sentiment = sentiment_score(finalcluster) \n",
    "        # for score in sentence_sentiment:\n",
    "        #     sentence_scores.append(score)\n",
    "    return(sentence_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is only for julie strain fans '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['sentence'].loc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:00<?, ?it/s]c:\\Users\\krish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:940: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.asarray(value)\n",
      "100%|██████████| 999/999 [04:39<00:00,  3.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through data\n",
    "review_scores_list = []\n",
    "\n",
    "#reviews\n",
    "for i in tqdm(range(0, n - 1)):\n",
    "    # sentences\n",
    "    review_scores = []\n",
    "    # for j in range(0, len(tokenized_data['sentence'].loc[i]) - 1):\n",
    "    for j in range(0, 1):\n",
    "        sentence_scores = []\n",
    "        sentence = tokenized_data['sentence'].loc[i][j]\n",
    "        pos = tokenized_data['pos'].loc[i][j]\n",
    "        token = tokenized_data['token'].loc[i][j] \n",
    "        if len(sentence.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # print(sentence)\n",
    "            doc = nlp(sentence)\n",
    "            try:\n",
    "                scores = find_relationships(doc, token, pos)\n",
    "                # print(scores)\n",
    "                if len(scores) != 0:\n",
    "                    review_scores.append(scores)\n",
    "            except:\n",
    "                continue\n",
    "        tokenized_data['scores'].iloc[i] = review_scores \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data.to_csv('data/output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
