{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import stanza\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import corpora\n",
    "stanza.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# initialise nlp pipeline\n",
    "nlp = stanza.Pipeline()\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'The book is too long, but the characters were good. Who knew Darth Vader was such a caring father?'\n",
    "# sample = 'The book is too long, but the characters were good.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review/text\n",
       "0  This is only for Julie Strain fans. It's a col...\n",
       "1  I don't care much for Dr. Seuss but after read...\n",
       "2  If people become the books they read and if \"t...\n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...\n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first n rows of review text\n",
    "reviews[['review/text']].iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this is only for julie strain fans., it's a c...\n",
       "Name: review/text, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase and tokenise\n",
    "data = reviews[['review/text']].iloc[:1].apply(lambda x: x.astype(str).str.lower())\n",
    "data = data['review/text'].apply(nltk.sent_tokenize)\n",
    "data.head()\n",
    "# text = reviews\n",
    "# text = text.lower()\n",
    "# text = nltk.sent_tokenize(text)\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is only for julie strain fans.',\n",
       " \"it's a collection of her photos -- about 80 pages worth with a nice section of paintings by olivia.if you're looking for heavy literary content, this isn't the place to find it -- there's only about 2 pages with text and everything else is photos.bottom line: if you only want one book, the six foot one ... is probably a better choice, however, if you like julie like i like julie, you won't go wrong on this one either.\"]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalfeatureList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score sentiment \n",
    "def sentiment_score(finalcluster):\n",
    "    scores = []\n",
    "    for pair in finalcluster:\n",
    "        # only look at valid pairs\n",
    "        if len(pair[1]) != 0:\n",
    "            score = sid.polarity_scores(''.join(pair[1]))\n",
    "            pair_score = [pair[0], score['compound']]\n",
    "            scores.append(pair_score)\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['section', 0.4215], ['juliestrain', 0.0], ['collection', 0.0]]\n"
     ]
    }
   ],
   "source": [
    "totalfeatureList = []\n",
    "for review in data:\n",
    "    sentence_scores = []\n",
    "    for sent in review:\n",
    "        word_list = nltk.word_tokenize(sent)\n",
    "        pos_list = nltk.pos_tag(word_list)\n",
    "\n",
    "        # word combiner\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(pos_list)-1):\n",
    "            if(pos_list[i][1]==\"NN\" and pos_list[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                newwordList.append(pos_list[i][0]+pos_list[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(pos_list[i][0])\n",
    "                if(i==len(pos_list)-2):\n",
    "                    newwordList.append(pos_list[i+1][0])\n",
    "        finaltxt = ' '.join(word for word in newwordList)\n",
    "\n",
    "\n",
    "        # \n",
    "        stop = set(stopwords.words('english'))\n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop]\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "\n",
    "\n",
    "        # relationship parser\n",
    "        doc = nlp(finaltxt)\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "\n",
    "        # possible features\n",
    "        featureList = []\n",
    "        categories = []\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "                totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "                categories.append(i[0])\n",
    "\n",
    "        # cluster together features and descriptors\n",
    "        fcluster = []\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "\n",
    "        # select only nouns\n",
    "        finalcluster = []\n",
    "        dic = {}\n",
    "        for i in featureList:\n",
    "            dic[i[0]] = i[1]\n",
    "        for i in fcluster:\n",
    "            if(dic[i[0]]==\"NN\"):\n",
    "                finalcluster.append(i)\n",
    "\n",
    "        # get sentence scores  \n",
    "        sentence_sentiment = sentiment_score(finalcluster) \n",
    "        \n",
    "        for score in sentence_sentiment:\n",
    "            sentence_scores.append(score)\n",
    "    \n",
    "    sentence_scores = sorted(sentence_scores, key=itemgetter(1), reverse = True)\n",
    "    print(sentence_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe3bbb772decd82072f87143d0eb4ba7cd317c875311c60d967c3986be5a5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
